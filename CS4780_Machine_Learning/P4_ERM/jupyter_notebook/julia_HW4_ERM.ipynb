{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Project 4: Empirical Risk Minimization</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "    <center>\n",
    "    <img src=\"./spam.jpeg\" width=\"200px\" />\n",
    "    </center>\n",
    "      <p><cite><center>\"One person's spam is another person's dinner.\"<br>\n",
    "       -- ancient German wisdon\n",
    "      </center></cite></p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Introduction</h3>\n",
    "\n",
    "<p>In this project you will be building an email spam filter.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>How to submit:</strong> You can submit your code using the red <strong>Submit</strong> button above. This button will send any code below surrounded by <strong>#&lt;GRADED&gt;</strong><strong>#&lt;/GRADED&gt;</strong> tags below to the autograder, which will then run several tests over your code. By clicking on the <strong>Details</strong> dropdown next to the Submit button, you will be able to view your submission report once the autograder has completed running. This submission report contains a summary of the tests you have failed or passed, as well as a log of any errors generated by your code when we ran it.\n",
    "\n",
    "Note that this may take a while depending on how long your code takes to run! Once your code is submitted you may navigate away from the page as you desire -- the most recent submission report will always be available from the Details menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Academic Integrity:</strong> We will be checking your code against other submissions in the class for logical redundancy. If you copy someone else's code and submit it with minor changes, we will know. These cheat detectors are quite hard to fool, so please don't try. We trust you all to submit your own work only; <em>please</em> don't let us down. If you do, we will pursue the strongest consequences available to us.\n",
    "                </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Getting Help:</strong> You are not alone!  If you find yourself stuck  on something, contact the course staff for help.  Office hours, section, and the <a href=\"https://piazza.com/class/iyag4nk2rsxsv\">Piazza</a> are there for your support; please use them.  If you can't make our office hours, let us know and we will schedule more.  We want these projects to be rewarding and instructional, not frustrating and demoralizing.  But, we don't know when or how to help unless you ask.  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Efficiency:</strong> You may run into the autograder time limit for this project (especially if you are competing in the competition). A few tips:</p>\n",
    "\n",
    "<ol>\n",
    "<li>Avoid for loops in the loss functions.</li>\n",
    "<li>Store large matrix multiplications in temporary variables if they need to be reused.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Computing derivatives</h3>\n",
    "\n",
    "<p>  Before you dive into the programming part of this assignment you will need to derive the gradients for several loss functions. \n",
    "    <b>You do not need to hand this part in for Project 4 but you will need to hand it in for Homework 3.</b> \n",
    "</p>\n",
    "\n",
    "<p>   Derive the gradient function for each of the following loss functions with respect to the weight vector $w$. Write down the gradient update (with stepsize $c$). <br>\n",
    "(Note that:    $\\|w\\|_2^2=w^\\top w$ and  $\\lambda$ is a  non-negative constant.)\n",
    "</p>\n",
    "\n",
    "<ol>\n",
    "    <li> Ridge Regression: ${\\cal L}(w)=\\frac{1}{n}\\sum_{i=1}^n (w^\\top x_i-y_i)^2+\\lambda \\|w\\|_2^2$ </li>\n",
    "    <li> Logistic Regression: ($y_i\\in\\{+1,-1\\}$): ${\\cal L}(w)=\\sum_{i=1}^n \\log(1+\\exp{(-y_i w^\\top x_i)})$ </li>\n",
    "    <li> Hinge loss: ($y_i\\in\\{+1,-1\\}$): ${\\cal L}(w)=\\sum_{i=1}^n \\max \\left(1-y_i(w^\\top x_i+b),0\\right)+\\lambda \\|w\\|_2^2$ </li>\n",
    "</ol>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building an email spam filter</h3>\n",
    "<p> You will now implement ridge loss and the Adagrad algorithm.\n",
    "   \n",
    "The function below loads in pre-processed email data, where emails are represented as bag-of-words vectors.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should have run this code before, but just in case\n",
    "try\n",
    "    using MAT\n",
    "catch\n",
    "    Pkg.add(\"MAT\")\n",
    "    Pkg.build(\"MAT\")\n",
    "end\n",
    "try\n",
    "    using PyPlot\n",
    "catch\n",
    "    Pkg.add(\"PyPlot\")\n",
    "    Pkg.build(\"PyPlot\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function extractfeaturesnaive(path,B)\n",
    "    # tokenize the email and hashes the symbols into a vector\n",
    "    femail=open(path);\n",
    "    v=zeros(B,1);              # initialize all-zeros feature vector\n",
    "    email=readstring(femail);\n",
    "    tokens=split(email);       # breaks for non-ascii characters\n",
    "    for token=split(email);\n",
    "        v[mod(hash(token),B)+1]=1;\n",
    "    end;\n",
    "    close(femail);\n",
    "    return(v);\n",
    "end;\n",
    "\n",
    "function loadspamdata(extractfeatures;B=512,path=\"../resource/lib/public/data_train/\")\n",
    "    print(path)\n",
    "\n",
    "    f=open(path*\"index\");\n",
    "    allemails=filter(s -> issubset(\" \",s),split(readstring(f),\"\\n\"));\n",
    "    xs=zeros(length(allemails),B);\n",
    "    ys=zeros(length(allemails),1);\n",
    "    for (i,line) âˆˆ enumerate(allemails)\n",
    "        label,filename=split(line,\" \")\n",
    "        ys[i]=(label==\"spam\").*2.0-1.0; # make labels +1 for \"spam\" and -1 for \"ham\"\n",
    "        xs[i,:]=extractfeatures(path*filename, B);\n",
    "    end;\n",
    "    close(f);\n",
    "    @printf(\"Loaded %i input emails.\\n\",length(ys));\n",
    "    return(xs,ys);\n",
    "end;\n",
    "\n",
    "X,Y=loadspamdata(extractfeaturesnaive);\n",
    "print(size(X));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is your training set. To evaluate your algorithm you should split it off into a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation\n",
    "n,d=size(X);\n",
    "cutoff=Int(ceil(0.8*n));\n",
    "itr=1:cutoff;    # indices of training samples\n",
    "ite=cutoff.+1:n; # indices of testing samples\n",
    "xTr=X[itr,:];\n",
    "yTr=Y[itr];\n",
    "xTv=X[ite,:];\n",
    "yTv=Y[ite];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This should generate a training data set <code>xTr</code>, <code>yTr</code> and a validation set <code>xTv</code>, <code>yTv</code> for you. (You can check what variables is in your environment with the <code>whos()</code> command.) \n",
    "</p>\n",
    "\n",
    "<p>It is now time to implement your classifiers. We will always use the Adagrad gradient descent algorithm, but with various loss functions. \n",
    "First implement the function <code>ridge</code> which computes the ridge regression loss and gradient for a particular data set <code>xTr</code>, <code>yTr</code> and a weight vector <code>w</code>. Make sure you don't forget to incorporate your regularization constant $\\lambda$. </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "function ridge(w,xTr,yTr,lmbda)\n",
    "    #\n",
    "    # INPUT:\n",
    "    # w     : d   dimensional weight vector\n",
    "    # xTr   : nxd dimensional matrix (each column is an input vector)\n",
    "    # yTr   : n   dimensional vector (each entry is a label)\n",
    "    # lmbda : regression constant (scalar)\n",
    "    #\n",
    "    # OUTPUTS:\n",
    "    # loss     : the total loss obtained with w on xTr and yTr (scalar)\n",
    "    # gradient : d dimensional gradient at w\n",
    "    #\n",
    "    \n",
    "    n,d=size(xTr);\n",
    "    \n",
    "    loss = 0;\n",
    "    gradient = zeros(d);\n",
    "    \n",
    "    ## fill in code here\n",
    "\n",
    "    return(loss[1],gradient[:]);\n",
    "end;\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>An  alternative to  deriving the gradient analytically is to estimate it numerically. This is very slow, but it is a convenient  way to check your code for correctness.  The following function  uses numerical differentiation to evaluate the correctness of ridge.  If your code is correct, the norm difference between the two should be very small (smaller than $10^{-5}$). \n",
    "Keep in mind that this only checks if the gradient corresponds to the loss, but not if the loss is correct. The function also plots an image of the gradient values (blue) and their estimates (red). If everything is correct, these two should be right on top of each other.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function numericalgradient(fun,w,e);\n",
    "    d=length(w);                   # get dimensionality\n",
    "    dh=zeros(d,1);                 # initialize numerical derivative\n",
    "    for i=1:d                      # go through dimensions\n",
    "        nw=w;                      # copy the weight vector\n",
    "        nw[i]+=e;                  # perturb dimension i\n",
    "        l1,temp=fun(nw);           # compute loss\n",
    "        nw[i]-=2*e;                # perturb dimension i again\n",
    "        l2,temp=fun(nw);           # compute loss\n",
    "        dh[i]=(l1[1]-l2[1])/(2*e); # the gradient is the slope of the loss\n",
    "    end;\n",
    "    return(dh[:]);\n",
    "end;\n",
    "\n",
    "function checkgrad(fun,w,e);\n",
    "    loss,dy=fun(w);                # evaluate symbolic gradient from fun()    \n",
    "    dh=numericalgradient(fun,w,e); # estimate gradient numerically from fun()\n",
    "    \n",
    "    ii=sortperm(dy);\n",
    "    ii=1:length(dy);\n",
    "    plot(dh[ii],\"bo\");\n",
    "    hold(true);\n",
    "    plot(dy[ii],\"r.\");\n",
    "    xlabel(\"Dimension\");\n",
    "    ylabel(\"Gradient value\");\n",
    "    legend([\"numeric\",\"symbolic\"]);\n",
    "\n",
    "    return(norm(dh-dy)/norm(dh+dy)); # return the norm of the difference scaled by the norm of the sum\n",
    "end;\n",
    "\n",
    "lmbda=0.1;     # set lambda arbitrarily\n",
    "d=size(xTr,2); # dimensionality of the input\n",
    "w=rand(d,1);   # evaluate loss on random vector\n",
    "ratio=checkgrad(w->ridge(w,xTr,yTr,lmbda),w,1e-05); # the a->fun(a) is an inline way to define a function with only a single argument.\n",
    "print(\"The norm ratio is $ratio.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Implement the function <code>adagrad</code> which performs adaptive gradient descent. \n",
    "Make sure to include the tolerance variable to stop early if the norm of the gradient is less than the tolerance value (you can use the function <code>norm(x)</code>). When the norm of the gradient is tiny it means that you have arrived at a minimum.  <br>\n",
    "The first parameter of <code>adagrad</code> is a function which takes a weight vector and returns loss and gradient.\n",
    "</p>                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "function adagrad(func,w,alpha,maxiter;delta=1e-02)\n",
    "    #\n",
    "    # INPUT:\n",
    "    # func    : function to minimize\n",
    "    #           (loss, gradient = func(w))\n",
    "    # w       : d dimensional initial weight vector \n",
    "    # alpha   : initial gradient descent stepsize (scalar)\n",
    "    # maxiter : maximum amount of iterations (scalar)\n",
    "    # delta   : if norm(gradient)<delta, it quits (scalar)\n",
    "    #\n",
    "    # OUTPUTS:\n",
    "    # \n",
    "    # w      : d dimensional final weight vector\n",
    "    # losses : vector containing loss at each iteration\n",
    "    #\n",
    "    \n",
    "    losses=zeros(maxiter);\n",
    "    epsilon=1e-06; # Adagrad epsilon term\n",
    "    \n",
    "    ## fill in code here\n",
    "\n",
    "    return(w[:],losses);\n",
    "end;\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w,losses=adagrad(w->ridge(w,xTr,yTr,lmbda),rand(size(xTr,2)),1,1000);\n",
    "\n",
    "using PyPlot\n",
    "figure()\n",
    "semilogy(losses,\"r-\");\n",
    "xlabel(\"gradient updates\");\n",
    "ylabel(\"loss\");\n",
    "title(\"Adagrad convergence\");\n",
    "@printf(\"Final loss: %e\",losses[end]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Write the (almost trivial) function <code>linclassify</code> which returns the predictions for a vector <code>w</code> and a data set <code>xTv</code>. (You can take it from a previous project.)</p>\n",
    "\n",
    "<p>After this you can check your training and validation accuracy by running the cell below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "\n",
    "function linclassify(w,xTr);\n",
    "    preds = zeros(size(xTr, 1));\n",
    "    \n",
    "    ## fill in code here\n",
    "    \n",
    "    return preds;\n",
    "end;\n",
    "#</GRADED>\n",
    "    \n",
    "# evaluate training accuracy\n",
    "preds=linclassify(w,xTr);\n",
    "\n",
    "trainingacc=mean(preds.==yTr);\n",
    "# evaluate testing accuracy\n",
    "preds=linclassify(w,xTv);\n",
    "validationacc=mean(preds.==yTv);\n",
    "@printf(\"Training accuracy %2.2f%%\\nValidation accuracy %2.2f%%\\n\",trainingacc*100,validationacc*100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now implement the two other loss functions, <code>logistic</code> and <code>hinge</code>. Start off with <code>logistic</code>:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "function logistic(w,xTr,yTr)\n",
    "    #\n",
    "    # INPUT:\n",
    "    # w     : d   dimensional weight vector\n",
    "    # xTr   : nxd dimensional matrix (each column is an input vector)\n",
    "    # yTr   : n   dimensional vector (each entry is a label)\n",
    "    #\n",
    "    # OUTPUTS:\n",
    "    # loss     : the total loss obtained with w on xTr and yTr (scalar)\n",
    "    # gradient : d dimensional gradient at w\n",
    "    #\n",
    "    \n",
    "    n,d=size(xTr);\n",
    "    \n",
    "    loss = 0;\n",
    "    gradient = zeros(d);\n",
    "    \n",
    "    ## fill in code here\n",
    "\n",
    "    return(loss[1],gradient[:]);\n",
    "end;\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>You can use the two cells below to test how well this loss function performs.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient sanity check\n",
    "d=size(xTr,2);\n",
    "w=rand(d,1);\n",
    "ratio=checkgrad(w->logistic(w,xTr,yTr),w,1e-05);\n",
    "print(\"The norm ratio is $ratio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using logistic loss\n",
    "w,losses=adagrad(w->logistic(w,xTr,yTr),rand(size(xTr,2)),1,1000);\n",
    "\n",
    "# evaluate training accuracy\n",
    "preds=linclassify(w,xTr);\n",
    "trainingacc=mean(preds.==yTr);\n",
    "# evaluate testing accuracy\n",
    "preds=linclassify(w,xTv);\n",
    "validationacc=mean(preds.==yTv);\n",
    "@printf(\"Training accuracy %2.2f%%\\nValidation accuracy %2.2f%%\\n\",trainingacc*100,validationacc*100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now implement <code>hinge</code>:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "function hinge(w,xTr,yTr,lmbda)\n",
    "    #\n",
    "    # INPUT:\n",
    "    # w     : d   dimensional weight vector\n",
    "    # xTr   : nxd dimensional matrix (each column is an input vector)\n",
    "    # yTr   : n   dimensional vector (each entry is a label)\n",
    "    # lmbda : regression constant (scalar)\n",
    "    #\n",
    "    # OUTPUTS:\n",
    "    # loss     : the total loss obtained with w on xTr and yTr (scalar)\n",
    "    # gradient : d dimensional gradient at w\n",
    "    #\n",
    "    \n",
    "    n,d=size(xTr);\n",
    "    \n",
    "    loss = 0;\n",
    "    gradient = zeros(d);\n",
    "    \n",
    "    ## fill in code here\n",
    "    \n",
    "    return(loss[1],gradient[:]);\n",
    "end;\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>You can use the two cells below to test how well this loss function performs.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient sanity check\n",
    "lmbda=0.1;\n",
    "d=size(xTr,2);\n",
    "w=rand(d,1);\n",
    "ratio=checkgrad(w->hinge(w,xTr,yTr,lmbda),w,1e-05);\n",
    "print(\"The norm ratio is $ratio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using hinge loss\n",
    "w,losses=adagrad(w->hinge(w,xTr,yTr,lmbda),rand(size(xTr,2)),1,1000);\n",
    "\n",
    "# evaluate training accuracy\n",
    "preds=linclassify(w,xTr);\n",
    "trainingacc=mean(preds.==yTr);\n",
    "# evaluate testing accuracy\n",
    "preds=linclassify(w,xTv);\n",
    "validationacc=mean(preds.==yTv);\n",
    "@printf(\"Training accuracy %2.2f%%\\nValidation accuracy %2.2f%%\\n\",trainingacc*100,validationacc*100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Competition <b>(Optional)</b></h3>\n",
    "\n",
    "<p>The competition for this assignment is split into two components:</p>\n",
    "\n",
    "<ol>\n",
    "<li><b>Feature Extraction</b>:\n",
    "Modify the function <code>extractfeaturescomp</code>.\n",
    "This function takes in a file path <code>path</code> and\n",
    "a feature dimension <code>B</code> and should output a feature vector of dimension <code>B</code>.\n",
    "The autograder will pass in a file path pointing to a file that contains an email,\n",
    "and set <code>B</code> = <code>feature_dimension</code>.\n",
    "We provide <code>extractfeaturesnaive</code> as an example.\n",
    "</li>\n",
    "<li><b>Model Training</b>:\n",
    "Modify the function <code>trainspamfiltercomp</code>.\n",
    "This function takes in training data <code>xTr</code> and training labels <code>yTr</code> and\n",
    "should output a weight vector <code>w</code> for linear classification.\n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "<p>Your model will be trained on the same training set above (loaded by <code>loadspamdata</code>), but we will test its accuracy on a secret dataset of emails.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "# dimensionality of feature vectors you want to use for the competition\n",
    "feature_dimension = 512;\n",
    "\n",
    "function extractfeaturescomp(path, B)\n",
    "    #\n",
    "    # INPUT:\n",
    "    # path : file path of email\n",
    "    # B    : dimensionality of feature vector\n",
    "    #\n",
    "    # OUTPUTS:\n",
    "    # x    : B dimensional vector\n",
    "    \n",
    "    #x = extractfeaturesnaive(path, B);\n",
    "      \n",
    "    # tokenize the email and hashes the symbols into a vector\n",
    "    femail=open(path);\n",
    "    x=zeros(B,1);              # initialize all-zeros feature vector\n",
    "    email=readstring(femail);\n",
    "    tokens=split(email);       # breaks for non-ascii characters\n",
    "    for token=split(email);\n",
    "        x[mod(hash(token),B)+1]=1;\n",
    "    end;\n",
    "    close(femail);\n",
    "    return(x);\n",
    "end;\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "function trainspamfiltercomp(xTr,yTr)\n",
    "    #\n",
    "    # INPUT:\n",
    "    # xTr : nxd dimensional matrix (each column is an input vector)\n",
    "    # yTr : d   dimensional vector (each entry is a label)\n",
    "    #\n",
    "    # OUTPUTS:\n",
    "    # w : d dimensional vector for linear classification\n",
    "    #\n",
    "    \n",
    "    w = rand(size(xTr, 2));\n",
    "\n",
    "    return w[:];\n",
    "end;\n",
    "#</GRADED>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
